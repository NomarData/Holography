<!DOCTYPE html>
<html lang="en">
	<head>
		<title>Instant Holograph</title>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, user-scalable=no, minimum-scale=1.0, maximum-scale=1.0">
        <link type="text/css" rel="stylesheet" href="main.css">
        <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.2/dist/tf.min.js"> </script> 
        <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/body-pix@2.0.5/dist/body-pix.min.js"></script> 
    </head>
    <body>  
        <div class="content">
        <video video autoplay playsinline class="hidden"></video>
        <div class="red-square">
            <div class="overlay">
                <p>When in AR mode, create a hologram by pointing at a person/face/photo and tap on the screen, it should appear gently. (Beware of camera framing, not too close)</p>
                <div class="linkG"><p><b><a href="https://github.com/Omodaka9375/Holography">https://github.com/Omodaka9375/Holography</a></b></p></div>
            </div>
        </div>
        </div>
        <div id="container"></div>
        <script type="x-shader/x-vertex" id="vertexshaderParticle">
            uniform sampler2D originalImage;
            uniform sampler2D depthMap;
            uniform float time;
            uniform vec3 userPos;
            uniform float width;
            uniform float height;
            uniform int optionMode;
            attribute vec2 uvs;
            varying vec2 vUv;
            varying float vDepth;
            float distancePoint = 3.;
            float localRadius = 0.3;
            const float M_PI = 3.1415926535897932384626433832795;

            float luminance(float r, float g, float b){
                return  (r * 0.3) + (g * 0.59) + (b * 0.11);
            }

            void main() {   
                vUv = uvs;
                float ratio = height/width;
                gl_PointSize = 19.;
                vec4 dMap = texture2D( depthMap, uvs) ;
                vec4 img = texture2D( originalImage, uvs) ;
                float lumi = luminance(img.r, img.g, img.b);
                vec3 pos = distancePoint * vec3(uvs.x - 0.5, (uvs.y - 0.5) * ratio - 0.25, 1. + lumi / 40.); 
                pos.xyz *= 0.65;  // Reduce long portrait sample (todo automatic)
                vDepth = dMap.r; 

                vec3 originalPhotoPos = vec3(position.x, position.y - 2., position.z + 6.); // Starting position
                vec3 interpolatedPos = mix(originalPhotoPos, pos, min( /*position.x*/ + (lumi + 0.2) / 10. * time * 15., 1.));

                // Particles Continuous animation
                // Depends on the distance to the viewer
                // vec3 camPos = viewMatrix[3].xyz;
                // float distToCam = distance(interpolatedPos, camPos /*cameraPosition*/ ) * 1.;
                // interpolatedPos += distToCam / 5.;
                interpolatedPos += (position / 60.) * (sin(position.y * time * 20.) * 2. )  / time; 
                // interpolatedPos += (position / 60.) * (position.y + sin(time * 0.2 * M_PI ) ); 
                 gl_PointSize +=  4. * sin(uvs.y + time * 0.5 * M_PI) ; 

                if(optionMode == 1){  // Mode for particles to circle at bottom
                    if(uvs.y < 0.1) {
                        float alpha = time * 6. + position.x * 10.;
                        float x = (localRadius + position.z / 5.) * cos(alpha);
                        float z = (localRadius + position.z / 5.) * sin(alpha);
                        interpolatedPos += vec3(x, interpolatedPos.y/8. + 0., z + 0.);
                        // gl_PointSize = 8.;
                    }  
                }  
                 
                gl_Position = projectionMatrix * modelViewMatrix * vec4(interpolatedPos, 1.0 );
            }
        </script>
        <script type="x-shader/x-fragment" id="fragmentshaderParticle">
            #extension GL_EXT_shader_texture_lod : enable
            #extension GL_OES_standard_derivatives : enable

            uniform float time;
            uniform sampler2D originalImage;
            uniform sampler2D depthMap;
            varying vec2 vUv;
            varying float vDepth;

            void main() {
                vec4 pointSpriteTexture = texture2D( originalImage, vUv );
                vec4 earthMap = texture2D( depthMap, vUv ) ;
                gl_FragColor =  pointSpriteTexture;
                // gl_FragColor.a = vDepth * 0.7;
                // if(vDepth < 0.5) gl_FragColor.a = 0.05 * (4. - time *2.);   // To make original pic with people out disappear
                float r = 0.0, delta = 0.0, alpha = 1.0;
                vec2 cxy = 2.0 * gl_PointCoord - 1.0;
                r = dot(cxy, cxy);
                // #ifdef GL_OES_standard_derivatives
                    delta = fwidth(r);
                    alpha = 1.0 - smoothstep(1.0 - delta, 1.0 + delta, r);
                // #endif
                
                gl_FragColor *= alpha;
            }
        </script>
        <script type="module">
            import * as THREE from './three.module.js';
            import { ARButton } from './ARButton.js';
            var renderer, scene, camera, controls;
            var time = 0;
            var particles;
            var shaderMaterialParticles;
            var textureSegmentation;
            var subsamplingFactor = 1;
            var textureOriginal = new THREE.Texture(); 
            var textureDepthMap = new THREE.Texture();
            // Photo
            var mediaStream;
            var controller;
            var video = document.querySelector('video');
            var net; // Model for inference

            async function loadModel(){
                // Load the model
                net = await bodyPix.load({
                        architecture: 'MobileNetV1',
                        outputStride: 16,
                        multiplier: 0.75,
                        quantBytes: 2,
                        internalResolution: 'medium',
                        segmentationThreshold: 0.7
                });
            }

            function init() {
                camera = new THREE.PerspectiveCamera( 75, window.innerWidth / window.innerHeight, 0, 1000 );
                scene = new THREE.Scene();
                renderer = new THREE.WebGLRenderer({antialias:true, alpha:true});
                renderer.xr.enabled = true;
                renderer.setPixelRatio( window.devicePixelRatio );
                renderer.setSize( window.innerWidth, window.innerHeight );

                let  container = document.getElementById( 'container' );
                container.appendChild( renderer.domElement );
                document.body.appendChild( ARButton.createButton( renderer ) );

                var uniforms = {
                    optionMode: {value: 1},
                    userPos: {value: camera.position},
                    width: {value: 1},
                    height: {value: 1},
                    time: {value: time},
                    originalImage: { value: textureOriginal },
                    depthMap: { value: textureDepthMap}
                };

                shaderMaterialParticles = new THREE.ShaderMaterial({
                    uniforms: uniforms,
                    vertexShader: document.getElementById( 'vertexshaderParticle' ).textContent,
                    fragmentShader: document.getElementById( 'fragmentshaderParticle' ).textContent,
                    depthTest: false,
                    transparent: true,
                    vertexColors: true
                });

                controller = renderer.xr.getController( 0 );
                controller.addEventListener( 'select',  onSelect );
                scene.add( controller );  

                getStream();
            }
            // Get a video stream from rear camera source.
            function getStream() {
                if (mediaStream) {
                    mediaStream.getTracks().forEach(track => {
                    track.stop();
                    });
                }
                getMedia();
            }
            
            async function getMedia() {
                var constraints = { video: { facingMode: { exact: 'environment'}}}

                if('mediaDevices' in navigator && 'getUserMedia' in navigator.mediaDevices){
                    try {
                        var stream = await navigator.mediaDevices.getUserMedia(constraints);
                        mediaStream = stream;
                        video.srcObject = stream;
                        video.classList.remove('hidden');
                    } catch(err) {
                        console.log('getUserMedia error: ', err);
                    }
                }
            }

            function grabFrame() {
                    var canvas = document.createElement('canvas');
                    const ctx = canvas.getContext('2d');          
                    canvas.width = video.videoWidth;
                    canvas.height = video.videoHeight;
                    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
                    textureOriginal = null;
                    textureOriginal = new THREE.Texture(canvas);
                    textureOriginal.needsUpdate = true;
                    video.hidden = true;
                    loadAndPredict(textureOriginal.image);
            }

            function onSelect() { grabFrame(); }

            function animate() { renderer.setAnimationLoop( render ); }

			function render() {
                time += 0.01;
                particles.material.uniforms.time.value = time ;
                renderer.render( scene, camera );
            }
            // Part TensorFlow https://github.com/tensorflow/tfjs-models/tree/master/body-pix
            async function loadAndPredict(img) {
                if(net == null){
                    console.log("loading model");
                    net = await bodyPix.load({
                        architecture: 'MobileNetV1',
                        outputStride: 16,
                        multiplier: 0.75,
                        quantBytes: 2,
                        internalResolution: 'medium',
                        segmentationThreshold: 0.7 
                    });
                }
                /**
                 * One of (see documentation below):
                 *   - net.segmentPerson
                 *   - net.segmentPersonParts
                 *   - net.segmentMultiPerson
                 *   - net.segmentMultiPersonParts
                 * See documentation below for details on each method.
                 */
                var segmentation = await net.segmentPerson(img);
                createTextureAndParticlesFromSegmentation(segmentation);
            }
            // Create Data texture using the segmentation array result AND THE PARTICLES (only on people)
            function createTextureAndParticlesFromSegmentation(seg){
                if(particles) scene.remove(particles);

                var size = seg.width * seg.height;
                var data = new Uint8Array( 3 * size );
                subsamplingFactor = 4;
                var w = seg.width;
                var h = seg.height;
                var geometry = new THREE.BufferGeometry();
                var positions = [];
                var uvs = [];
                var u,v;
                var nbParticles = 0;
                var nAdded = 0;

                for ( var i = 0; i < size; i ++ ) {
                    // Texture part
                    var stride = i * 3;
                    data[ stride ] = seg.data[i] * 255;
                    data[ stride + 1 ] = seg.data[i];
                    data[ stride + 2 ] = seg.data[i];
                    // Particles part. We test if People or not to create a particle
                    if(seg.data[i] > 0){
                        if(nAdded % subsamplingFactor == 0){
                            nbParticles++;
                            u = (i % w ) / w; 
                            v = Math.floor(i / w) / h;
                            positions.push(Math.random(), Math.random(), Math.random());
                            uvs.push(u, 1. - v);  // flip y
                        }
                        nAdded++;
                    }
                }
                // Part particles
                geometry.setAttribute( 'position', new THREE.Float32BufferAttribute( positions, 3 ) );
                geometry.setAttribute( 'uvs', new THREE.Float32BufferAttribute( uvs, 2 ) );

                particles = new THREE.Points( geometry, shaderMaterialParticles );
                particles.material.uniforms.originalImage.value = textureOriginal;
                particles.position.z = -4;
  
                particles.frustumCulled = false;
                particles.material.uniforms.width.value = w ;
                particles.material.uniforms.height.value = h ;
                // Part texture
                textureSegmentation = new THREE.DataTexture( data, seg.width, seg.height, THREE.RGBFormat );
                textureSegmentation.flipY = true;
                time = 0;
                particles.material.uniforms.time.value = time ;
                particles.material.uniforms.depthMap.value = textureSegmentation;
                scene.add( particles );
                console.log("Particles count: ", nbParticles);                
                animate();
            }
            loadModel();
            init(); 
        </script>
    </body>
</html>
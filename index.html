<!DOCTYPE html>
<html lang="en">
	<head>
		<title>Holograms</title>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, user-scalable=no, minimum-scale=1.0, maximum-scale=1.0">
        <link type="text/css" rel="stylesheet" href="main.css">
        
        <!-- Load TensorFlow.js -->
        <!-- <script src="./tf.min.js"></script> -->
        <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs/dist/tf.min.js"> </script> 
        <!-- Load BodyPix -->
        <!--  <script src="./body-pix.min.js"></script> -->
        <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/body-pix@2.0.5/dist/body-pix.min.js"></script> 



        <!-- 
        <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.2"></script>
        <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/body-pix@2.0"></script> -->

    </head> 
    <body>  


        <script type="x-shader/x-vertex" id="vertexshaderParticle">
            uniform sampler2D originalImage;
            uniform sampler2D depthMap;
            uniform float time;
            uniform vec3 userPos;
            uniform float width;
            uniform float height;
            uniform int optionMode;
            attribute vec2 uvs;
            varying vec2 vUv;
            varying float vDepth;
            float distancePoint = 3.;
            float localRadius = 0.3;
            const float M_PI = 3.1415926535897932384626433832795;

            float luminance(float r, float g, float b){
                return  (r * 0.3) + (g * 0.59) + (b * 0.11);
            }

            void main() {
                
                vUv = uvs;
                float ratio = height/width;
                gl_PointSize = 19.;
                vec4 dMap = texture2D( depthMap, uvs) ;
                vec4 img = texture2D( originalImage, uvs) ;
                float lumi = luminance(img.r, img.g, img.b);
                vec3 pos = distancePoint * vec3(uvs.x - 0.5, (uvs.y - 0.5) * ratio - 0.25, 1. + lumi / 40.); 
                pos.xyz *= 0.65;  // Reduce long portrait sample (todo automatic)
                vDepth = dMap.r; 

                vec3 originalPhotoPos = vec3(position.x, position.y - 2., position.z + 6.); // Starting position
                vec3 interpolatedPos = mix(originalPhotoPos, pos, min( /*position.x*/ + (lumi + 0.2) / 10. * time * 15., 1.));

                // Particles Continuous animation
                // Depends on the distance to the viewer
                // vec3 camPos = viewMatrix[3].xyz;
                // float distToCam = distance(interpolatedPos, camPos /*cameraPosition*/ ) * 1.;
                // interpolatedPos += distToCam / 5.;
                interpolatedPos += (position / 60.) * (sin(position.y * time * 20.) * 2. )  / time; 
                // interpolatedPos += (position / 60.) * (position.y + sin(time * 0.2 * M_PI ) ); 
                 gl_PointSize +=  4. * sin(uvs.y + time * 0.5 * M_PI) ; 
                

                if(optionMode == 1){  // Mode for particles to circle at bottom
                    if(uvs.y < 0.1) {
                        float alpha = time * 6. + position.x * 10.;
                        float x = (localRadius + position.z / 5.) * cos(alpha);
                        float z = (localRadius + position.z / 5.) * sin(alpha);
                        interpolatedPos += vec3(x, interpolatedPos.y/8. + 0., z + 0.);
                        // gl_PointSize = 8.;
                    }  
                }  
                 
                gl_Position = projectionMatrix * modelViewMatrix * vec4(interpolatedPos, 1.0 );
            }

        </script>

        <script type="x-shader/x-fragment" id="fragmentshaderParticle">
            #extension GL_EXT_shader_texture_lod : enable
            #extension GL_OES_standard_derivatives : enable

            uniform float time;
            uniform sampler2D originalImage;
            uniform sampler2D depthMap;
            varying vec2 vUv;
            varying float vDepth;


            void main() {

                vec4 pointSpriteTexture = texture2D( originalImage, vUv );
                vec4 earthMap = texture2D( depthMap, vUv ) ;
                gl_FragColor =  pointSpriteTexture;
                // gl_FragColor.a = vDepth * 0.7;
                // if(vDepth < 0.5) gl_FragColor.a = 0.05 * (4. - time *2.);   // To make original pic with people out disappear
                float r = 0.0, delta = 0.0, alpha = 1.0;
                vec2 cxy = 2.0 * gl_PointCoord - 1.0;
                r = dot(cxy, cxy);
                // #ifdef GL_OES_standard_derivatives
                    delta = fwidth(r);
                    alpha = 1.0 - smoothstep(1.0 - delta, 1.0 + delta, r);
                // #endif
                
                gl_FragColor *= alpha;
  

            }

        </script>

        <script type="module">

            import * as THREE from './three.module.js';
            import { ARButton } from './ARButton.js';
            var renderer, scene, camera, controls;
            var time = 0;
            var particles;
            var texture;
            var shaderMaterialParticles;
            var textureSegmentation;
            var textureName = "im_7.jpg"; 
            var depthMapName = "depthMapFriends.png";
            var subsamplingFactor = 1;

            var textureOriginal = new THREE.Texture(); // new THREE.TextureLoader().load( textureName );
            var textureDepthMap = new THREE.Texture(); // new THREE.TextureLoader().load( depthMapName);

            // Photo
            var constraints;
            var imageCapture;
            var container;
            var controller;
            var net;
            var grabFrameButton = document.querySelector('button#grabFrame');
            var videoSelect = document.querySelector('select#videoSource');
            var video = document.querySelector('video');
            //grabFrameButton.onclick = grabFrame;
           // videoSelect.onchange = getStream;
          

           // var net; // Model for inference

            async function loadModel(){
                // Load the model
                net = await bodyPix.load(
                    {
                        architecture: 'MobileNetV1',
                        outputStride: 16,
                        multiplier: 0.5,
                        quantBytes: 2,
                        internalResolution: 'medium',
                        segmentationThreshold: 0.7
                        }
                    );
            }

            function gotStream(stream) {
                console.log('getUserMedia() got stream: ', stream);

              //  print('WATCH: ' + str(len(stream.getVideoTracks())))
                imageCapture = new ImageCapture(stream.getVideoTracks()[0]);
 
            }

            // Get an ImageBitmap from the currently selected camera source and
            // display this with a canvas element.
            function grabFrame() {
                imageCapture.track.enabled = true;
                imageCapture.grabFrame().then(function(imageBitmap) {
                    console.log('Grabbed frame:', imageBitmap);
                          
                    canvas.width = imageBitmap.width;
                    canvas.height = imageBitmap.height;
                    canvas.getContext('2d').drawImage(imageBitmap, 0, 0);
                    canvas.classList.remove('hidden');
                    textureOriginal = new THREE.Texture(canvas);
                    textureOriginal.needsUpdate = true;

                    // createParticles(textureOriginal);
                    loadAndPredict(textureOriginal.image);
                    
                   
                }).catch(function(error) {
                    console.log('grabFrame() error: ', error);
                });
            }


            function init() {

                container = document.createElement( 'div' );
				document.body.appendChild( container );
                scene = new THREE.Scene();
                
                camera = new THREE.PerspectiveCamera( 75, window.innerWidth / window.innerHeight, 0.1, 1000 );
                
                renderer = new THREE.WebGLRenderer({antialias:true, alpha:true});
                
                renderer.setPixelRatio( window.devicePixelRatio );
                renderer.setSize( window.innerWidth, window.innerHeight );
                renderer.xr.enabled = true;
   
                container.appendChild( renderer.domElement );
                document.body.appendChild( ARButton.createButton( renderer ) );
                
                var uniforms = {
                    optionMode: {value: 1},
                    userPos: {value: camera.position},
                    width: {value: 1},
                    height: {value: 1},
                    time: {value: time},
                    originalImage: { value: textureOriginal },
                    depthMap: { value: textureDepthMap}
				};

				shaderMaterialParticles = new THREE.ShaderMaterial( {

					uniforms: uniforms,
					vertexShader: document.getElementById( 'vertexshaderParticle' ).textContent,
					fragmentShader: document.getElementById( 'fragmentshaderParticle' ).textContent,
					depthTest: false,
					transparent: true,
					vertexColors: true
                } );
              
                var geometryC = new THREE.BoxGeometry( 0.1,0.1,0.1 );
                var materialC = new THREE.MeshBasicMaterial( {color: 0x00ff00, side: THREE.DoubleSide, transparent:true, opacity:0.3} );

                // grabFrame and show extracte silouhette
                function onSelect() {
                    grabFrame();
                }

                controller = renderer.xr.getController( 0 );
				controller.addEventListener( 'select', onSelect );
                scene.add( controller );

                var videoSource = "777c810d69597cf54aff909a1cd006dfaa3e2b31cd1be326faa98b50f75e9848"; // "f8d0c84d41cd5014401d4737b2a34be3bfc9abac0288377cba5186cdaa4d18dc"
                constraints = {
                    video: {deviceId: videoSource ? {exact: videoSource} : undefined}
                };
                
 
                getMedia(constraints);
            }

            navigator.mediaDevices.enumerateDevices()
                .then(gotDevices)
                .catch(error => {
                    console.log('enumerateDevices() error: ', error);
                });

            // From the list of media devices available, set up the camera source <select>,
            // then get a video stream from the default camera source.
            function gotDevices(deviceInfos) {
                for (var i = 0; i !== deviceInfos.length; ++i) {
                    var deviceInfo = deviceInfos[i];
                    console.log('Found media input or output device: ', deviceInfo);
                    if (deviceInfo.kind === 'videoinput') {
                    //option.text = deviceInfo.label || 'Camera ' + (videoSelect.length + 1);
                    console.log('Found media input or output device: ', deviceInfo);
                    }
                }
            }

            loadModel();
            init();
           // getMedia();

            async function getMedia(constraints) {
                let stream = null;

                try {
                    stream = await navigator.mediaDevices.getUserMedia(constraints);
                    gotStream(stream)
                    /* use the stream */
                } catch(err) {
                    /* handle the error */
                    console.log('getUserMedia error: ', err);
                }
            }

            function animate() {

                renderer.setAnimationLoop( render );
            }



			function render() {
                time += 0.01;
                //if(time > 10) time = 0;
                particles.material.uniforms.time.value = time ;
               // particles.material.uniforms.userPos.value = camera.position; // Normally passed by default...
                renderer.render( scene, camera );
                // console.log(camera);
            }



            // Part TensorFlow https://github.com/tensorflow/tfjs-models/tree/master/body-pix
            async function loadAndPredict(img) {
/*                   console.log("Loading model");
                   const net = await bodyPix.load(
                    {
                        architecture: 'MobileNetV1',
                        outputStride: 16,
                        multiplier: 0.5,
                        quantBytes: 2

                        }
                    ); */
                
                /**
                 * One of (see documentation below):
                 *   - net.segmentPerson
                 *   - net.segmentPersonParts
                 *   - net.segmentMultiPerson
                 *   - net.segmentMultiPersonParts
                 * See documentation below for details on each method.
                 */
                var segmentation = await net.segmentPerson(img, {
                    flipHorizontal: false,
                    internalResolution: 'medium',
                    segmentationThreshold: 0.7
                }); 
                console.log(segmentation);

                createTextureAndParticlesFromSegmentation(segmentation);
            }

            // Create Data texture using the segmentation array result AND THE PARTICLES (only on people)
            function createTextureAndParticlesFromSegmentation(seg){
                

                if(particles) scene.remove(particles);

                var size = seg.width * seg.height;
                var data = new Uint8Array( 3 * size );
                subsamplingFactor = 4;
                var w = seg.width;
                var h = seg.height;
                var geometry = new THREE.BufferGeometry();
                var positions = [];
                var uvs = [];
                var u,v;
                var nbParticles = 0;
                var nAdded = 0;

                for ( var i = 0; i < size; i ++ ) {

                    // Texture part
                    var stride = i * 3;
                    data[ stride ] = seg.data[i] * 255;
                    data[ stride + 1 ] = seg.data[i];
                    data[ stride + 2 ] = seg.data[i];

                    // Particles part. We test if People or not to create a particle
                    if(seg.data[i] > 0){
                        if(nAdded % subsamplingFactor == 0){
                            nbParticles++;
                            u = (i % w ) / w; 
                            v = Math.floor(i / w) / h;
                            positions.push(Math.random(), Math.random(), Math.random());
                            uvs.push(u, 1. - v);  // flip y
                        }
                        nAdded++;
                    }
                }


                // Part particles
                geometry.setAttribute( 'position', new THREE.Float32BufferAttribute( positions, 3 ) );
                geometry.setAttribute( 'uvs', new THREE.Float32BufferAttribute( uvs, 2 ) );

                particles = new THREE.Points( geometry, shaderMaterialParticles );
                particles.material.uniforms.originalImage.value = textureOriginal;
                particles.position.z = -4;
                // particles.position.y -= 0.5;
                particles.frustumCulled = false;
                particles.material.uniforms.width.value = w ;
                particles.material.uniforms.height.value = h ;

                // Part texture
                textureSegmentation = new THREE.DataTexture( data, seg.width, seg.height, THREE.RGBFormat );
                textureSegmentation.flipY = true;
                time = 0;
                particles.material.uniforms.time.value = time ;
                particles.material.uniforms.depthMap.value = textureSegmentation;

                
                scene.add( particles );
                console.log("Particles count: ", nbParticles);
                                
                animate();

            }
          

        </script>


    </body>
</html>